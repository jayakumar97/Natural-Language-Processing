{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "KseH1-Zf9qJu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KseH1-Zf9qJu",
    "outputId": "8f7b34a3-aa10-4d67-9a2f-ebe5a3fc9773"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.6)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.51.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.3.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (63.4.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (23.3.3)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.21.5)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (15.0.6.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\rjkhe\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e04f0f6",
   "metadata": {
    "id": "4e04f0f6"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(50)\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc9e8806",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc9e8806",
    "outputId": "b7ff68f2-a34c-404e-b7e7-8c354f91cb93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e158f01e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "e158f01e",
    "outputId": "1257366d-f291-4bd1-95f3-8c7fc8c5303d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00f3a072",
   "metadata": {
    "id": "00f3a072"
   },
   "outputs": [],
   "source": [
    "#Reading the learning Data\n",
    "text = open(\"shakespeare.txt\", 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "#preprocessing\n",
    "text=text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2f2c3ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2f2c3ae",
    "outputId": "36986544-9c44-46a9-b3ae-b253d91472a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the sonnets\n",
      "\n",
      "by william shakespeare\n",
      "\n",
      "from fairest creatures we desire increase,\n",
      "that thereby beauty's rose might never die,\n",
      "but as the riper should by time decease,\n",
      "his tender heir might bear his memory:\n",
      "but thou contracted to thine own bright eyes,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sample Text Data\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "962604a8",
   "metadata": {
    "id": "962604a8"
   },
   "outputs": [],
   "source": [
    "#Vocabulary\n",
    "vocab = sorted(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08b57f42",
   "metadata": {
    "id": "08b57f42"
   },
   "outputs": [],
   "source": [
    "get_id_from_char = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d056865",
   "metadata": {
    "id": "1d056865",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_char_from_id = tf.keras.layers.StringLookup(vocabulary=get_id_from_char.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e78d0bf2",
   "metadata": {
    "id": "e78d0bf2"
   },
   "outputs": [],
   "source": [
    "def generate_text(ids):\n",
    "  return tf.strings.reduce_join(get_char_from_id(ids), axis=-1)\n",
    "\n",
    "def lambda_split(sequence):\n",
    "    return sequence[:-1], sequence[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2b37ebd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2b37ebd",
    "outputId": "bb046c37-0ddb-4ab7-d0c9-fc668e5a91c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([32 20 17 ... 17 26 16], shape=(93677,), dtype=int64)\n",
      "93677\n"
     ]
    }
   ],
   "source": [
    "text_ids = get_id_from_char(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "print(text_ids)\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "129539fb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "129539fb",
    "outputId": "39a3895c-1fe8-4c83-f38b-dc62950f744d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training Samples =  31213\n",
      "Vocab Size 38 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(text_ids)\n",
    "stride = 3\n",
    "window = 40\n",
    "## creating dataset with window size and stride\n",
    "dataset = dataset.window(size=window+1,shift=stride, drop_remainder=True)\n",
    "\n",
    "## Function to flat the dataset from the datasets\n",
    "def flat_function(batch):\n",
    "    return batch.batch(window+1, drop_remainder=True)\n",
    "\n",
    "## Use above function to apply on the loaded dataset\n",
    "dataset_sequence = dataset.flat_map(flat_function)\n",
    "\n",
    "count = 0\n",
    "for x in dataset_sequence:\n",
    "  count+=1\n",
    "print(\"Number of training Samples = \",count)\n",
    "print(f'Vocab Size {len(vocab)} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b8b80f8",
   "metadata": {
    "id": "4b8b80f8"
   },
   "outputs": [],
   "source": [
    "C = tf.constant(len(vocab))\n",
    "\n",
    "#Function to Generate the input and target sequences and perform one hot encoding\n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    input_text = tf.one_hot(\n",
    "        input_text, C, on_value = 1.0, off_value = 0.0, axis =-1)\n",
    "    target_text = tf.one_hot(\n",
    "        target_text, C, on_value = 1.0, off_value = 0.0, axis =-1)\n",
    "    \n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3434fcc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3434fcc",
    "outputId": "5c7efb82-d535-49cd-a0c2-de27c47100a5"
   },
   "outputs": [],
   "source": [
    "dataset = dataset_sequence.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58042096",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "58042096",
    "outputId": "3524f4cf-33eb-4f2c-d22f-f5ba2533c24e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'the sonnets\\n\\nby william shakespeare\\n\\nfro'\n",
      "\n",
      "One Hot encoding representation for above text tf.Tensor(\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]], shape=(40, 38), dtype=float32)\n",
      "\n",
      "\n",
      "Target: b'he sonnets\\n\\nby william shakespeare\\n\\nfrom'\n",
      "\n",
      "One Hot encoding representation for above text tf.Tensor(\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]], shape=(40, 38), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "for sample_input, sample_target in dataset.take(1):\n",
    "    ids = tf.argmax(sample_input, axis=1)\n",
    "    print(\"Input :\", generate_text(ids).numpy())\n",
    "    print(\"\\nOne Hot encoding representation for above text\",sample_input)\n",
    "    ids = tf.argmax(sample_target, axis=1)\n",
    "    print(\"\\n\\nTarget:\", generate_text(ids).numpy())\n",
    "    print(\"\\nOne Hot encoding representation for above text\",sample_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b6cb530",
   "metadata": {
    "id": "2b6cb530"
   },
   "outputs": [],
   "source": [
    "batch = 32\n",
    "buffer = 10000\n",
    "dataset = dataset_sequence.map(lambda_split)\n",
    "dataset = (dataset.shuffle(buffer).batch(batch, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2622d2f9",
   "metadata": {
    "id": "2622d2f9"
   },
   "outputs": [],
   "source": [
    "# Vocabulary Size for embedding layer\n",
    "vocabulary_size = len(get_id_from_char.get_vocabulary())\n",
    "\n",
    "# dimension for embedding layer\n",
    "embedding_dimensions = 128\n",
    "\n",
    "# LSTM units\n",
    "LSTM_units = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3305fa7c",
   "metadata": {
    "id": "3305fa7c"
   },
   "outputs": [],
   "source": [
    "class LSTMModel(tf.keras.Model):\n",
    "  def __init__(self, vocabulary_size, embedding_dimension, LSTM_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocabulary_size, embedding_dimension)\n",
    "    self.lstm = tf.keras.layers.LSTM(LSTM_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocabulary_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.lstm.get_initial_state(x)\n",
    "    x= self.lstm(x, initial_state=states, training=training)\n",
    "    states=x[1:]\n",
    "    x=x[0]\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "103368d1",
   "metadata": {
    "id": "103368d1"
   },
   "outputs": [],
   "source": [
    "#Creating LSTMMODEL Object\n",
    "model = LSTMModel(vocabulary_size=vocabulary_size,embedding_dimension=embedding_dimensions,LSTM_units=LSTM_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1652a8e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1652a8e3",
    "outputId": "ff86a1ad-3e8b-44e9-b979-08c91524b776"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 40, 39) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for sample_input_batch, sample_target_batch in dataset.take(1):\n",
    "    sample_batch_predictions = model(sample_input_batch)\n",
    "    print(sample_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "323fea5c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "323fea5c",
    "outputId": "500214ad-e28c-4bfe-af37-b1e6efe7a90f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lstm_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  4992      \n",
      "                                                                 \n",
      " lstm (LSTM)                 multiple                  394240    \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  10023     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 409,255\n",
      "Trainable params: 409,255\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22271858",
   "metadata": {
    "id": "22271858"
   },
   "outputs": [],
   "source": [
    "indexes = tf.random.categorical(sample_batch_predictions[0], num_samples=1)\n",
    "indexes = tf.squeeze(indexes, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28eae890",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28eae890",
    "outputId": "493424ce-d4dd-48be-931f-c0722b03e936"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b'om nature hath not made for store,\\nharsh'\n",
      "\n",
      "Next Char Predictions:\n",
      " b'iy.!fdz;:ot  dpbou[UNK]hdv:cgiucr(:i)mfqkiyw'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", generate_text(sample_input_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", generate_text(indexes).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41c3c678",
   "metadata": {
    "id": "41c3c678"
   },
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b131025d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b131025d",
    "outputId": "3a953c1c-171c-4606-a848-ddc14517dcb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (32, 40, 39)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(3.6637626, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "mean_loss = loss(sample_target_batch, sample_batch_predictions)\n",
    "print(\"Prediction shape: \", sample_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "629e1b74",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "629e1b74",
    "outputId": "8da243aa-8421-4298-dae7-892811c22da6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.007835"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fbab1731",
   "metadata": {
    "id": "fbab1731"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4877053b",
   "metadata": {
    "id": "4877053b"
   },
   "outputs": [],
   "source": [
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "010fba38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "010fba38",
    "outputId": "bfd562d4-a00f-4be1-ffdd-fa07e04f983c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "975/975 [==============================] - 14s 11ms/step - loss: 2.0492\n",
      "Epoch 2/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 1.5475\n",
      "Epoch 3/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 1.3537\n",
      "Epoch 4/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 1.1996\n",
      "Epoch 5/1000\n",
      "975/975 [==============================] - 11s 10ms/step - loss: 1.0597\n",
      "Epoch 6/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.9355\n",
      "Epoch 7/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.8334\n",
      "Epoch 8/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.7502\n",
      "Epoch 9/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.6853\n",
      "Epoch 10/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.6334\n",
      "Epoch 11/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.5924\n",
      "Epoch 12/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.5600\n",
      "Epoch 13/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.5342\n",
      "Epoch 14/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.5115\n",
      "Epoch 15/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.4944\n",
      "Epoch 16/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.4794\n",
      "Epoch 17/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.4665\n",
      "Epoch 18/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.4556\n",
      "Epoch 19/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.4463\n",
      "Epoch 20/1000\n",
      "975/975 [==============================] - 8s 8ms/step - loss: 0.4375\n",
      "Epoch 21/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.4301\n",
      "Epoch 22/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.4233\n",
      "Epoch 23/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.4178\n",
      "Epoch 24/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.4120\n",
      "Epoch 25/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.4073\n",
      "Epoch 26/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.4020\n",
      "Epoch 27/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.3977\n",
      "Epoch 28/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.3937\n",
      "Epoch 29/1000\n",
      "975/975 [==============================] - 8s 8ms/step - loss: 0.3902\n",
      "Epoch 30/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.3866\n",
      "Epoch 31/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3838\n",
      "Epoch 32/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.3811\n",
      "Epoch 33/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3776\n",
      "Epoch 34/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3753\n",
      "Epoch 35/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.3726\n",
      "Epoch 36/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3699\n",
      "Epoch 37/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3676\n",
      "Epoch 38/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3655\n",
      "Epoch 39/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.3634\n",
      "Epoch 40/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3620\n",
      "Epoch 41/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3597\n",
      "Epoch 42/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3577\n",
      "Epoch 43/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.3562\n",
      "Epoch 44/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.3541\n",
      "Epoch 45/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3533\n",
      "Epoch 46/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3514\n",
      "Epoch 47/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.3502\n",
      "Epoch 48/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.3486\n",
      "Epoch 49/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.3471\n",
      "Epoch 50/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3457\n",
      "Epoch 51/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.3448\n",
      "Epoch 52/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3436\n",
      "Epoch 53/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3422\n",
      "Epoch 54/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3409\n",
      "Epoch 55/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.3401\n",
      "Epoch 56/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3387\n",
      "Epoch 57/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3376\n",
      "Epoch 58/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3365\n",
      "Epoch 59/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.3360\n",
      "Epoch 60/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.3345\n",
      "Epoch 61/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3333\n",
      "Epoch 62/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3327\n",
      "Epoch 63/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.3319\n",
      "Epoch 64/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.3311\n",
      "Epoch 65/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3306\n",
      "Epoch 66/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3295\n",
      "Epoch 67/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.3282\n",
      "Epoch 68/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3275\n",
      "Epoch 69/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3271\n",
      "Epoch 70/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.3268\n",
      "Epoch 71/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.3256\n",
      "Epoch 72/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3250\n",
      "Epoch 73/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3235\n",
      "Epoch 74/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.3233\n",
      "Epoch 75/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3232\n",
      "Epoch 76/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3219\n",
      "Epoch 77/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.3219\n",
      "Epoch 78/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.3201\n",
      "Epoch 79/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.3206\n",
      "Epoch 80/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3202\n",
      "Epoch 81/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.3192\n",
      "Epoch 82/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3182\n",
      "Epoch 83/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.3182\n",
      "Epoch 84/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.3171\n",
      "Epoch 85/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.3173\n",
      "Epoch 86/1000\n",
      "975/975 [==============================] - 10s 10ms/step - loss: 0.3162\n",
      "Epoch 87/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3164\n",
      "Epoch 88/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.3155\n",
      "Epoch 89/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3148\n",
      "Epoch 90/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3145\n",
      "Epoch 91/1000\n",
      "975/975 [==============================] - 8s 8ms/step - loss: 0.3138\n",
      "Epoch 92/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.3137\n",
      "Epoch 93/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3128\n",
      "Epoch 94/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3124\n",
      "Epoch 95/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3115\n",
      "Epoch 96/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3113\n",
      "Epoch 97/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3106\n",
      "Epoch 98/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3115\n",
      "Epoch 99/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3099\n",
      "Epoch 100/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.3099\n",
      "Epoch 101/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3095\n",
      "Epoch 102/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3093\n",
      "Epoch 103/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.3088\n",
      "Epoch 104/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.3081\n",
      "Epoch 105/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3081\n",
      "Epoch 106/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3074\n",
      "Epoch 107/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.3075\n",
      "Epoch 108/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.3069\n",
      "Epoch 109/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3060\n",
      "Epoch 110/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3059\n",
      "Epoch 111/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.3056\n",
      "Epoch 112/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.3052\n",
      "Epoch 113/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3052\n",
      "Epoch 114/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.3041\n",
      "Epoch 115/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3037\n",
      "Epoch 116/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3045\n",
      "Epoch 117/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3038\n",
      "Epoch 118/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3033\n",
      "Epoch 119/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.3024\n",
      "Epoch 120/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3033\n",
      "Epoch 121/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3022\n",
      "Epoch 122/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.3024\n",
      "Epoch 123/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.3019\n",
      "Epoch 124/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3014\n",
      "Epoch 125/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3004\n",
      "Epoch 126/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.3009\n",
      "Epoch 127/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3008\n",
      "Epoch 128/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.3010\n",
      "Epoch 129/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.3000\n",
      "Epoch 130/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2997\n",
      "Epoch 131/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.2997\n",
      "Epoch 132/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2985\n",
      "Epoch 133/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2997\n",
      "Epoch 134/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2982\n",
      "Epoch 135/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.2986\n",
      "Epoch 136/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2986\n",
      "Epoch 137/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2979\n",
      "Epoch 138/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2979\n",
      "Epoch 139/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2976\n",
      "Epoch 140/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2969\n",
      "Epoch 141/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2972\n",
      "Epoch 142/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.2966\n",
      "Epoch 143/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2961\n",
      "Epoch 144/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2965\n",
      "Epoch 145/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2958\n",
      "Epoch 146/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2953\n",
      "Epoch 147/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2952\n",
      "Epoch 148/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2959\n",
      "Epoch 149/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2955\n",
      "Epoch 150/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2947\n",
      "Epoch 151/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2946\n",
      "Epoch 152/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2949\n",
      "Epoch 153/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2943\n",
      "Epoch 154/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2938\n",
      "Epoch 155/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2944\n",
      "Epoch 156/1000\n",
      "975/975 [==============================] - 8s 8ms/step - loss: 0.2931\n",
      "Epoch 157/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.2931\n",
      "Epoch 158/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2934\n",
      "Epoch 159/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2932\n",
      "Epoch 160/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2930\n",
      "Epoch 161/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.2926\n",
      "Epoch 162/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2924\n",
      "Epoch 163/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2918\n",
      "Epoch 164/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2926\n",
      "Epoch 165/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2918\n",
      "Epoch 166/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2916\n",
      "Epoch 167/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2914\n",
      "Epoch 168/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2911\n",
      "Epoch 169/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2913\n",
      "Epoch 170/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2910\n",
      "Epoch 171/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2905\n",
      "Epoch 172/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2911\n",
      "Epoch 173/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.2903\n",
      "Epoch 174/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2902\n",
      "Epoch 175/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2905\n",
      "Epoch 176/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2906\n",
      "Epoch 177/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2900\n",
      "Epoch 178/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2899\n",
      "Epoch 179/1000\n",
      "975/975 [==============================] - 8s 8ms/step - loss: 0.2890\n",
      "Epoch 180/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2890\n",
      "Epoch 181/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2890\n",
      "Epoch 182/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2891\n",
      "Epoch 183/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2900\n",
      "Epoch 184/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.2882\n",
      "Epoch 185/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2885\n",
      "Epoch 186/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2890\n",
      "Epoch 187/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2880\n",
      "Epoch 188/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.2880\n",
      "Epoch 189/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2883\n",
      "Epoch 190/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2876\n",
      "Epoch 191/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2881\n",
      "Epoch 192/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2870\n",
      "Epoch 193/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2873\n",
      "Epoch 194/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2878\n",
      "Epoch 195/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2870\n",
      "Epoch 196/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2866\n",
      "Epoch 197/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2870\n",
      "Epoch 198/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2864\n",
      "Epoch 199/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2866\n",
      "Epoch 200/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2862\n",
      "Epoch 201/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2865\n",
      "Epoch 202/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2861\n",
      "Epoch 203/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2864\n",
      "Epoch 204/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2857\n",
      "Epoch 205/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2859\n",
      "Epoch 206/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2857\n",
      "Epoch 207/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2851\n",
      "Epoch 208/1000\n",
      "975/975 [==============================] - 8s 8ms/step - loss: 0.2860\n",
      "Epoch 209/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2856\n",
      "Epoch 210/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2853\n",
      "Epoch 211/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2850\n",
      "Epoch 212/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2853\n",
      "Epoch 213/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2846\n",
      "Epoch 214/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2852\n",
      "Epoch 215/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2849\n",
      "Epoch 216/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2842\n",
      "Epoch 217/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2848\n",
      "Epoch 218/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2840\n",
      "Epoch 219/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2844\n",
      "Epoch 220/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2841\n",
      "Epoch 221/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2838\n",
      "Epoch 222/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2836\n",
      "Epoch 223/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2833\n",
      "Epoch 224/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2834\n",
      "Epoch 225/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2841\n",
      "Epoch 226/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.2830\n",
      "Epoch 227/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2830\n",
      "Epoch 228/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2831\n",
      "Epoch 229/1000\n",
      "975/975 [==============================] - 8s 8ms/step - loss: 0.2829\n",
      "Epoch 230/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2830\n",
      "Epoch 231/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.2828\n",
      "Epoch 232/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2823\n",
      "Epoch 233/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2826\n",
      "Epoch 234/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2828\n",
      "Epoch 235/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.2828\n",
      "Epoch 236/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2821\n",
      "Epoch 237/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2822\n",
      "Epoch 238/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2816\n",
      "Epoch 239/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2824\n",
      "Epoch 240/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2818\n",
      "Epoch 241/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2816\n",
      "Epoch 242/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2822\n",
      "Epoch 243/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2815\n",
      "Epoch 244/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2814\n",
      "Epoch 245/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2811\n",
      "Epoch 246/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2809\n",
      "Epoch 247/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2814\n",
      "Epoch 248/1000\n",
      "975/975 [==============================] - 10s 10ms/step - loss: 0.2807\n",
      "Epoch 249/1000\n",
      "975/975 [==============================] - 18s 15ms/step - loss: 0.2810\n",
      "Epoch 250/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2806\n",
      "Epoch 251/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2808\n",
      "Epoch 252/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.2807\n",
      "Epoch 253/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2810\n",
      "Epoch 254/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2804\n",
      "Epoch 255/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2801\n",
      "Epoch 256/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2810\n",
      "Epoch 257/1000\n",
      "975/975 [==============================] - 11s 10ms/step - loss: 0.2803\n",
      "Epoch 258/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2801\n",
      "Epoch 259/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2804\n",
      "Epoch 260/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2806\n",
      "Epoch 261/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2807\n",
      "Epoch 262/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2789\n",
      "Epoch 263/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.2803\n",
      "Epoch 264/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2796\n",
      "Epoch 265/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2793\n",
      "Epoch 266/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2799\n",
      "Epoch 267/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.2785\n",
      "Epoch 268/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2800\n",
      "Epoch 269/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2793\n",
      "Epoch 270/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2792\n",
      "Epoch 271/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2788\n",
      "Epoch 272/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2788\n",
      "Epoch 273/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2790\n",
      "Epoch 274/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2783\n",
      "Epoch 275/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2791\n",
      "Epoch 276/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.2789\n",
      "Epoch 277/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2786\n",
      "Epoch 278/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2794\n",
      "Epoch 279/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2782\n",
      "Epoch 280/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2784\n",
      "Epoch 281/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2778\n",
      "Epoch 282/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2779\n",
      "Epoch 283/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2785\n",
      "Epoch 284/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2778\n",
      "Epoch 285/1000\n",
      "975/975 [==============================] - 11s 10ms/step - loss: 0.2777\n",
      "Epoch 286/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2781\n",
      "Epoch 287/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2778\n",
      "Epoch 288/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2779\n",
      "Epoch 289/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2775\n",
      "Epoch 290/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2776\n",
      "Epoch 291/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2771\n",
      "Epoch 292/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2774\n",
      "Epoch 293/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2773\n",
      "Epoch 294/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2785\n",
      "Epoch 295/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2771\n",
      "Epoch 296/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2771\n",
      "Epoch 297/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2767\n",
      "Epoch 298/1000\n",
      "975/975 [==============================] - 8s 8ms/step - loss: 0.2779\n",
      "Epoch 299/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2767\n",
      "Epoch 300/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2768\n",
      "Epoch 301/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2772\n",
      "Epoch 302/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2764\n",
      "Epoch 303/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2765\n",
      "Epoch 304/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2768\n",
      "Epoch 305/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2767\n",
      "Epoch 306/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2766\n",
      "Epoch 307/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2762\n",
      "Epoch 308/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2755\n",
      "Epoch 309/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2769\n",
      "Epoch 310/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2759\n",
      "Epoch 311/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2762\n",
      "Epoch 312/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2766\n",
      "Epoch 313/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2761\n",
      "Epoch 314/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2755\n",
      "Epoch 315/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2756\n",
      "Epoch 316/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2759\n",
      "Epoch 317/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.2757\n",
      "Epoch 318/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2751\n",
      "Epoch 319/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2754\n",
      "Epoch 320/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2764\n",
      "Epoch 321/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2753\n",
      "Epoch 322/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2752\n",
      "Epoch 323/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2758\n",
      "Epoch 324/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2754\n",
      "Epoch 325/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2761\n",
      "Epoch 326/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2745\n",
      "Epoch 327/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2749\n",
      "Epoch 328/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2748\n",
      "Epoch 329/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2750\n",
      "Epoch 330/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2753\n",
      "Epoch 331/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2753\n",
      "Epoch 332/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2748\n",
      "Epoch 333/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2744\n",
      "Epoch 334/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2752\n",
      "Epoch 335/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2746\n",
      "Epoch 336/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2740\n",
      "Epoch 337/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2747\n",
      "Epoch 338/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2739\n",
      "Epoch 339/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2747\n",
      "Epoch 340/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2743\n",
      "Epoch 341/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2751\n",
      "Epoch 342/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2736\n",
      "Epoch 343/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2745\n",
      "Epoch 344/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2753\n",
      "Epoch 345/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2733\n",
      "Epoch 346/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2743\n",
      "Epoch 347/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2742\n",
      "Epoch 348/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2740\n",
      "Epoch 349/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2735\n",
      "Epoch 350/1000\n",
      "975/975 [==============================] - 8s 8ms/step - loss: 0.2746\n",
      "Epoch 351/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.2737\n",
      "Epoch 352/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2737\n",
      "Epoch 353/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2740\n",
      "Epoch 354/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2733\n",
      "Epoch 355/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2733\n",
      "Epoch 356/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2738\n",
      "Epoch 357/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2735\n",
      "Epoch 358/1000\n",
      "975/975 [==============================] - 8s 8ms/step - loss: 0.2728\n",
      "Epoch 359/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2733\n",
      "Epoch 360/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2734\n",
      "Epoch 361/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2727\n",
      "Epoch 362/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2732\n",
      "Epoch 363/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2733\n",
      "Epoch 364/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2728\n",
      "Epoch 365/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2731\n",
      "Epoch 366/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2731\n",
      "Epoch 367/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2726\n",
      "Epoch 368/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2721\n",
      "Epoch 369/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2734\n",
      "Epoch 370/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2718\n",
      "Epoch 371/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2729\n",
      "Epoch 372/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2721\n",
      "Epoch 373/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2722\n",
      "Epoch 374/1000\n",
      "975/975 [==============================] - 8s 8ms/step - loss: 0.2728\n",
      "Epoch 375/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2727\n",
      "Epoch 376/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2723\n",
      "Epoch 377/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2723\n",
      "Epoch 378/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2722\n",
      "Epoch 379/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.2721\n",
      "Epoch 380/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2729\n",
      "Epoch 381/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2718\n",
      "Epoch 382/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2720\n",
      "Epoch 383/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2719\n",
      "Epoch 384/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2723\n",
      "Epoch 385/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2721\n",
      "Epoch 386/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2717\n",
      "Epoch 387/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2719\n",
      "Epoch 388/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2718\n",
      "Epoch 389/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2721\n",
      "Epoch 390/1000\n",
      "975/975 [==============================] - 8s 8ms/step - loss: 0.2717\n",
      "Epoch 391/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2717\n",
      "Epoch 392/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.2710\n",
      "Epoch 393/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2718\n",
      "Epoch 394/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2721\n",
      "Epoch 395/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2719\n",
      "Epoch 396/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.2715\n",
      "Epoch 397/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2712\n",
      "Epoch 398/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2720\n",
      "Epoch 399/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2709\n",
      "Epoch 400/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2713\n",
      "Epoch 401/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2714\n",
      "Epoch 402/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2716\n",
      "Epoch 403/1000\n",
      "975/975 [==============================] - 8s 8ms/step - loss: 0.2707\n",
      "Epoch 404/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.2714\n",
      "Epoch 405/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2706\n",
      "Epoch 406/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2714\n",
      "Epoch 407/1000\n",
      "975/975 [==============================] - 8s 8ms/step - loss: 0.2714\n",
      "Epoch 408/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.2712\n",
      "Epoch 409/1000\n",
      "975/975 [==============================] - 11s 10ms/step - loss: 0.2710\n",
      "Epoch 410/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2705\n",
      "Epoch 411/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2713\n",
      "Epoch 412/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.2709\n",
      "Epoch 413/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2705\n",
      "Epoch 414/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2712\n",
      "Epoch 415/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2701\n",
      "Epoch 416/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2707\n",
      "Epoch 417/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2707\n",
      "Epoch 418/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2703\n",
      "Epoch 419/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2705\n",
      "Epoch 420/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.2702\n",
      "Epoch 421/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2709\n",
      "Epoch 422/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2700\n",
      "Epoch 423/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2699\n",
      "Epoch 424/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2703\n",
      "Epoch 425/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2697\n",
      "Epoch 426/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2698\n",
      "Epoch 427/1000\n",
      "975/975 [==============================] - 8s 8ms/step - loss: 0.2706\n",
      "Epoch 428/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.2698\n",
      "Epoch 429/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2702\n",
      "Epoch 430/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2697\n",
      "Epoch 431/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2698\n",
      "Epoch 432/1000\n",
      "975/975 [==============================] - 8s 8ms/step - loss: 0.2697\n",
      "Epoch 433/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2697\n",
      "Epoch 434/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2692\n",
      "Epoch 435/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2699\n",
      "Epoch 436/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2692\n",
      "Epoch 437/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2695\n",
      "Epoch 438/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2693\n",
      "Epoch 439/1000\n",
      "975/975 [==============================] - 11s 10ms/step - loss: 0.2698\n",
      "Epoch 440/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2691\n",
      "Epoch 441/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2699\n",
      "Epoch 442/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2696\n",
      "Epoch 443/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2699\n",
      "Epoch 444/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2700\n",
      "Epoch 445/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2684\n",
      "Epoch 446/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2694\n",
      "Epoch 447/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2693\n",
      "Epoch 448/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2696\n",
      "Epoch 449/1000\n",
      "975/975 [==============================] - 8s 7ms/step - loss: 0.2690\n",
      "Epoch 450/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2692\n",
      "Epoch 451/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2690\n",
      "Epoch 452/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2692\n",
      "Epoch 453/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2685\n",
      "Epoch 454/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2699\n",
      "Epoch 455/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2684\n",
      "Epoch 456/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2687\n",
      "Epoch 457/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2687\n",
      "Epoch 458/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.2688\n",
      "Epoch 459/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2692\n",
      "Epoch 460/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2685\n",
      "Epoch 461/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2687\n",
      "Epoch 462/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.2684\n",
      "Epoch 463/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2691\n",
      "Epoch 464/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2680\n",
      "Epoch 465/1000\n",
      "975/975 [==============================] - 8s 8ms/step - loss: 0.2683\n",
      "Epoch 466/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2684\n",
      "Epoch 467/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2691\n",
      "Epoch 468/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2681\n",
      "Epoch 469/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2686\n",
      "Epoch 470/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2684\n",
      "Epoch 471/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2687\n",
      "Epoch 472/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2671\n",
      "Epoch 473/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2684\n",
      "Epoch 474/1000\n",
      "975/975 [==============================] - 8s 8ms/step - loss: 0.2681\n",
      "Epoch 475/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2690\n",
      "Epoch 476/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2682\n",
      "Epoch 477/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2682\n",
      "Epoch 478/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2682\n",
      "Epoch 479/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2677\n",
      "Epoch 480/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2676\n",
      "Epoch 481/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2680\n",
      "Epoch 482/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2675\n",
      "Epoch 483/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2677\n",
      "Epoch 484/1000\n",
      "975/975 [==============================] - 9s 7ms/step - loss: 0.2682\n",
      "Epoch 485/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2674\n",
      "Epoch 486/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2682\n",
      "Epoch 487/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2669\n",
      "Epoch 488/1000\n",
      "975/975 [==============================] - 8s 8ms/step - loss: 0.2677\n",
      "Epoch 489/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2681\n",
      "Epoch 490/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2681\n",
      "Epoch 491/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2678\n",
      "Epoch 492/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2675\n",
      "Epoch 493/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2667\n",
      "Epoch 494/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2670\n",
      "Epoch 495/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2691\n",
      "Epoch 496/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2669\n",
      "Epoch 497/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2670\n",
      "Epoch 498/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2675\n",
      "Epoch 499/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2671\n",
      "Epoch 500/1000\n",
      "975/975 [==============================] - 8s 8ms/step - loss: 0.2673\n",
      "Epoch 501/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2670\n",
      "Epoch 502/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2676\n",
      "Epoch 503/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2675\n",
      "Epoch 504/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2674\n",
      "Epoch 505/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2666\n",
      "Epoch 506/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2670\n",
      "Epoch 507/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2678\n",
      "Epoch 508/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2672\n",
      "Epoch 509/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2668\n",
      "Epoch 510/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2667\n",
      "Epoch 511/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2664\n",
      "Epoch 512/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2666\n",
      "Epoch 513/1000\n",
      "975/975 [==============================] - 8s 8ms/step - loss: 0.2661\n",
      "Epoch 514/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2670\n",
      "Epoch 515/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2672\n",
      "Epoch 516/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2664\n",
      "Epoch 517/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2675\n",
      "Epoch 518/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2674\n",
      "Epoch 519/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2661\n",
      "Epoch 520/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2670\n",
      "Epoch 521/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2659\n",
      "Epoch 522/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2667\n",
      "Epoch 523/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2668\n",
      "Epoch 524/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2665\n",
      "Epoch 525/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2664\n",
      "Epoch 526/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2666\n",
      "Epoch 527/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2664\n",
      "Epoch 528/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2667\n",
      "Epoch 529/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2662\n",
      "Epoch 530/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2661\n",
      "Epoch 531/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2663\n",
      "Epoch 532/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2669\n",
      "Epoch 533/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2665\n",
      "Epoch 534/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2659\n",
      "Epoch 535/1000\n",
      "975/975 [==============================] - 8s 8ms/step - loss: 0.2661\n",
      "Epoch 536/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2663\n",
      "Epoch 537/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2663\n",
      "Epoch 538/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2652\n",
      "Epoch 539/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2662\n",
      "Epoch 540/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2662\n",
      "Epoch 541/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2659\n",
      "Epoch 542/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2661\n",
      "Epoch 543/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2661\n",
      "Epoch 544/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2656\n",
      "Epoch 545/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2661\n",
      "Epoch 546/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2657\n",
      "Epoch 547/1000\n",
      "975/975 [==============================] - 8s 8ms/step - loss: 0.2672\n",
      "Epoch 548/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2650\n",
      "Epoch 549/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2657\n",
      "Epoch 550/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2661\n",
      "Epoch 551/1000\n",
      "975/975 [==============================] - 8s 8ms/step - loss: 0.2651\n",
      "Epoch 552/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2664\n",
      "Epoch 553/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2651\n",
      "Epoch 554/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2667\n",
      "Epoch 555/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2655\n",
      "Epoch 556/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2656\n",
      "Epoch 557/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2657\n",
      "Epoch 558/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2647\n",
      "Epoch 559/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2656\n",
      "Epoch 560/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2658\n",
      "Epoch 561/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2652\n",
      "Epoch 562/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2656\n",
      "Epoch 563/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2656\n",
      "Epoch 564/1000\n",
      "975/975 [==============================] - 8s 8ms/step - loss: 0.2651\n",
      "Epoch 565/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2658\n",
      "Epoch 566/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2649\n",
      "Epoch 567/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2653\n",
      "Epoch 568/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2647\n",
      "Epoch 569/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2647\n",
      "Epoch 570/1000\n",
      "975/975 [==============================] - 10s 10ms/step - loss: 0.2653\n",
      "Epoch 571/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2655\n",
      "Epoch 572/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2652\n",
      "Epoch 573/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2646\n",
      "Epoch 574/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2648\n",
      "Epoch 575/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2656\n",
      "Epoch 576/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2648\n",
      "Epoch 577/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2655\n",
      "Epoch 578/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2649\n",
      "Epoch 579/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2648\n",
      "Epoch 580/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2639\n",
      "Epoch 581/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2654\n",
      "Epoch 582/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2653\n",
      "Epoch 583/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2644\n",
      "Epoch 584/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2647\n",
      "Epoch 585/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2653\n",
      "Epoch 586/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2649\n",
      "Epoch 587/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2641\n",
      "Epoch 588/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2649\n",
      "Epoch 589/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2650\n",
      "Epoch 590/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2646\n",
      "Epoch 591/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2642\n",
      "Epoch 592/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2653\n",
      "Epoch 593/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2647\n",
      "Epoch 594/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2641\n",
      "Epoch 595/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2643\n",
      "Epoch 596/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2654\n",
      "Epoch 597/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2641\n",
      "Epoch 598/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2644\n",
      "Epoch 599/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2648\n",
      "Epoch 600/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2635\n",
      "Epoch 601/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2643\n",
      "Epoch 602/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2642\n",
      "Epoch 603/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2647\n",
      "Epoch 604/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2640\n",
      "Epoch 605/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2643\n",
      "Epoch 606/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2640\n",
      "Epoch 607/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2643\n",
      "Epoch 608/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2647\n",
      "Epoch 609/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2639\n",
      "Epoch 610/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2654\n",
      "Epoch 611/1000\n",
      "975/975 [==============================] - 8s 8ms/step - loss: 0.2628\n",
      "Epoch 612/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2644\n",
      "Epoch 613/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2637\n",
      "Epoch 614/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2648\n",
      "Epoch 615/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2640\n",
      "Epoch 616/1000\n",
      "975/975 [==============================] - 8s 8ms/step - loss: 0.2645\n",
      "Epoch 617/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2641\n",
      "Epoch 618/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2632\n",
      "Epoch 619/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2644\n",
      "Epoch 620/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2643\n",
      "Epoch 621/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2637\n",
      "Epoch 622/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2637\n",
      "Epoch 623/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2639\n",
      "Epoch 624/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2645\n",
      "Epoch 625/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2637\n",
      "Epoch 626/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2633\n",
      "Epoch 627/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2638\n",
      "Epoch 628/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2634\n",
      "Epoch 629/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2642\n",
      "Epoch 630/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2637\n",
      "Epoch 631/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2638\n",
      "Epoch 632/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2637\n",
      "Epoch 633/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2634\n",
      "Epoch 634/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2634\n",
      "Epoch 635/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2642\n",
      "Epoch 636/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2638\n",
      "Epoch 637/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2624\n",
      "Epoch 638/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2628\n",
      "Epoch 639/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2640\n",
      "Epoch 640/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2632\n",
      "Epoch 641/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2634\n",
      "Epoch 642/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2638\n",
      "Epoch 643/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2634\n",
      "Epoch 644/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2628\n",
      "Epoch 645/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2633\n",
      "Epoch 646/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2627\n",
      "Epoch 647/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2629\n",
      "Epoch 648/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2637\n",
      "Epoch 649/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2635\n",
      "Epoch 650/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2634\n",
      "Epoch 651/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2635\n",
      "Epoch 652/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2629\n",
      "Epoch 653/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2635\n",
      "Epoch 654/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2632\n",
      "Epoch 655/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2628\n",
      "Epoch 656/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2632\n",
      "Epoch 657/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2633\n",
      "Epoch 658/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2626\n",
      "Epoch 659/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2631\n",
      "Epoch 660/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2638\n",
      "Epoch 661/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2627\n",
      "Epoch 662/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2632\n",
      "Epoch 663/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2625\n",
      "Epoch 664/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2626\n",
      "Epoch 665/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2638\n",
      "Epoch 666/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2622\n",
      "Epoch 667/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2632\n",
      "Epoch 668/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2629\n",
      "Epoch 669/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2636\n",
      "Epoch 670/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2625\n",
      "Epoch 671/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2619\n",
      "Epoch 672/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2629\n",
      "Epoch 673/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2631\n",
      "Epoch 674/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2624\n",
      "Epoch 675/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2630\n",
      "Epoch 676/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2628\n",
      "Epoch 677/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2624\n",
      "Epoch 678/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2628\n",
      "Epoch 679/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2630\n",
      "Epoch 680/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2628\n",
      "Epoch 681/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2625\n",
      "Epoch 682/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2626\n",
      "Epoch 683/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2622\n",
      "Epoch 684/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2623\n",
      "Epoch 685/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2625\n",
      "Epoch 686/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2621\n",
      "Epoch 687/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2632\n",
      "Epoch 688/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2625\n",
      "Epoch 689/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2621\n",
      "Epoch 690/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2621\n",
      "Epoch 691/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2623\n",
      "Epoch 692/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2628\n",
      "Epoch 693/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2627\n",
      "Epoch 694/1000\n",
      "975/975 [==============================] - 10s 10ms/step - loss: 0.2627\n",
      "Epoch 695/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2617\n",
      "Epoch 696/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2632\n",
      "Epoch 697/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2623\n",
      "Epoch 698/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2614\n",
      "Epoch 699/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2626\n",
      "Epoch 700/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2618\n",
      "Epoch 701/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2621\n",
      "Epoch 702/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2618\n",
      "Epoch 703/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2616\n",
      "Epoch 704/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2625\n",
      "Epoch 705/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2627\n",
      "Epoch 706/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2627\n",
      "Epoch 707/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2618\n",
      "Epoch 708/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2624\n",
      "Epoch 709/1000\n",
      "975/975 [==============================] - 11s 8ms/step - loss: 0.2615\n",
      "Epoch 710/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2620\n",
      "Epoch 711/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2623\n",
      "Epoch 712/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2615\n",
      "Epoch 713/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2623\n",
      "Epoch 714/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2620\n",
      "Epoch 715/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2617\n",
      "Epoch 716/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2618\n",
      "Epoch 717/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2621\n",
      "Epoch 718/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2622\n",
      "Epoch 719/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2619\n",
      "Epoch 720/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2619\n",
      "Epoch 721/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2626\n",
      "Epoch 722/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2612\n",
      "Epoch 723/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2619\n",
      "Epoch 724/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2611\n",
      "Epoch 725/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2626\n",
      "Epoch 726/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2618\n",
      "Epoch 727/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2616\n",
      "Epoch 728/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2617\n",
      "Epoch 729/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2618\n",
      "Epoch 730/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2615\n",
      "Epoch 731/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2616\n",
      "Epoch 732/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2613\n",
      "Epoch 733/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2628\n",
      "Epoch 734/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2613\n",
      "Epoch 735/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2606\n",
      "Epoch 736/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2618\n",
      "Epoch 737/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2623\n",
      "Epoch 738/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2614\n",
      "Epoch 739/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2617\n",
      "Epoch 740/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2612\n",
      "Epoch 741/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2618\n",
      "Epoch 742/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2614\n",
      "Epoch 743/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2609\n",
      "Epoch 744/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2618\n",
      "Epoch 745/1000\n",
      "975/975 [==============================] - 11s 8ms/step - loss: 0.2611\n",
      "Epoch 746/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2610\n",
      "Epoch 747/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2613\n",
      "Epoch 748/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2612\n",
      "Epoch 749/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2614\n",
      "Epoch 750/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2613\n",
      "Epoch 751/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2611\n",
      "Epoch 752/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2621\n",
      "Epoch 753/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2611\n",
      "Epoch 754/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2606\n",
      "Epoch 755/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2609\n",
      "Epoch 756/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2610\n",
      "Epoch 757/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2614\n",
      "Epoch 758/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2609\n",
      "Epoch 759/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2618\n",
      "Epoch 760/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2606\n",
      "Epoch 761/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2619\n",
      "Epoch 762/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2600\n",
      "Epoch 763/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2602\n",
      "Epoch 764/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2614\n",
      "Epoch 765/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2611\n",
      "Epoch 766/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2609\n",
      "Epoch 767/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2615\n",
      "Epoch 768/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2607\n",
      "Epoch 769/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2617\n",
      "Epoch 770/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2606\n",
      "Epoch 771/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2605\n",
      "Epoch 772/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2617\n",
      "Epoch 773/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2609\n",
      "Epoch 774/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2607\n",
      "Epoch 775/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2617\n",
      "Epoch 776/1000\n",
      "975/975 [==============================] - 11s 8ms/step - loss: 0.2603\n",
      "Epoch 777/1000\n",
      "975/975 [==============================] - 10s 10ms/step - loss: 0.2606\n",
      "Epoch 778/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2604\n",
      "Epoch 779/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2600\n",
      "Epoch 780/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2610\n",
      "Epoch 781/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2611\n",
      "Epoch 782/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2599\n",
      "Epoch 783/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2610\n",
      "Epoch 784/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2612\n",
      "Epoch 785/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2608\n",
      "Epoch 786/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2609\n",
      "Epoch 787/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2609\n",
      "Epoch 788/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2615\n",
      "Epoch 789/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2594\n",
      "Epoch 790/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2619\n",
      "Epoch 791/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2601\n",
      "Epoch 792/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2615\n",
      "Epoch 793/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2599\n",
      "Epoch 794/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2608\n",
      "Epoch 795/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2599\n",
      "Epoch 796/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2612\n",
      "Epoch 797/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2602\n",
      "Epoch 798/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2600\n",
      "Epoch 799/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2603\n",
      "Epoch 800/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2602\n",
      "Epoch 801/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2617\n",
      "Epoch 802/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2605\n",
      "Epoch 803/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2600\n",
      "Epoch 804/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2612\n",
      "Epoch 805/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2598\n",
      "Epoch 806/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2607\n",
      "Epoch 807/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2605\n",
      "Epoch 808/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2601\n",
      "Epoch 809/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2603\n",
      "Epoch 810/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2601\n",
      "Epoch 811/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2603\n",
      "Epoch 812/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2600\n",
      "Epoch 813/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2612\n",
      "Epoch 814/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2601\n",
      "Epoch 815/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2595\n",
      "Epoch 816/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2605\n",
      "Epoch 817/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2604\n",
      "Epoch 818/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2607\n",
      "Epoch 819/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2601\n",
      "Epoch 820/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2602\n",
      "Epoch 821/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2605\n",
      "Epoch 822/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2594\n",
      "Epoch 823/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2598\n",
      "Epoch 824/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2607\n",
      "Epoch 825/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2604\n",
      "Epoch 826/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2594\n",
      "Epoch 827/1000\n",
      "975/975 [==============================] - 11s 10ms/step - loss: 0.2603\n",
      "Epoch 828/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2601\n",
      "Epoch 829/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2597\n",
      "Epoch 830/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2602\n",
      "Epoch 831/1000\n",
      "975/975 [==============================] - 10s 10ms/step - loss: 0.2591\n",
      "Epoch 832/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2599\n",
      "Epoch 833/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2606\n",
      "Epoch 834/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2598\n",
      "Epoch 835/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2604\n",
      "Epoch 836/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2604\n",
      "Epoch 837/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2597\n",
      "Epoch 838/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2592\n",
      "Epoch 839/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2596\n",
      "Epoch 840/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2603\n",
      "Epoch 841/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2602\n",
      "Epoch 842/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2598\n",
      "Epoch 843/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2599\n",
      "Epoch 844/1000\n",
      "975/975 [==============================] - 10s 10ms/step - loss: 0.2604\n",
      "Epoch 845/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2602\n",
      "Epoch 846/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2591\n",
      "Epoch 847/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2597\n",
      "Epoch 848/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2600\n",
      "Epoch 849/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2599\n",
      "Epoch 850/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2597\n",
      "Epoch 851/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2599\n",
      "Epoch 852/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2591\n",
      "Epoch 853/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2589\n",
      "Epoch 854/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2591\n",
      "Epoch 855/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2606\n",
      "Epoch 856/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2588\n",
      "Epoch 857/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2599\n",
      "Epoch 858/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2607\n",
      "Epoch 859/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2586\n",
      "Epoch 860/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2589\n",
      "Epoch 861/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2601\n",
      "Epoch 862/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2597\n",
      "Epoch 863/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2589\n",
      "Epoch 864/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2599\n",
      "Epoch 865/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2589\n",
      "Epoch 866/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2592\n",
      "Epoch 867/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2596\n",
      "Epoch 868/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2596\n",
      "Epoch 869/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2594\n",
      "Epoch 870/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2592\n",
      "Epoch 871/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2590\n",
      "Epoch 872/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2590\n",
      "Epoch 873/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2594\n",
      "Epoch 874/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2589\n",
      "Epoch 875/1000\n",
      "975/975 [==============================] - 11s 10ms/step - loss: 0.2593\n",
      "Epoch 876/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2591\n",
      "Epoch 877/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2588\n",
      "Epoch 878/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2592\n",
      "Epoch 879/1000\n",
      "975/975 [==============================] - 11s 10ms/step - loss: 0.2590\n",
      "Epoch 880/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2595\n",
      "Epoch 881/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2592\n",
      "Epoch 882/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2591\n",
      "Epoch 883/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2590\n",
      "Epoch 884/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2599\n",
      "Epoch 885/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2595\n",
      "Epoch 886/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2589\n",
      "Epoch 887/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2592\n",
      "Epoch 888/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2595\n",
      "Epoch 889/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2583\n",
      "Epoch 890/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2595\n",
      "Epoch 891/1000\n",
      "975/975 [==============================] - 11s 10ms/step - loss: 0.2585\n",
      "Epoch 892/1000\n",
      "975/975 [==============================] - 10s 10ms/step - loss: 0.2588\n",
      "Epoch 893/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2597\n",
      "Epoch 894/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2591\n",
      "Epoch 895/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2586\n",
      "Epoch 896/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2588\n",
      "Epoch 897/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2595\n",
      "Epoch 898/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2586\n",
      "Epoch 899/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2597\n",
      "Epoch 900/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2578\n",
      "Epoch 901/1000\n",
      "975/975 [==============================] - 11s 10ms/step - loss: 0.2589\n",
      "Epoch 902/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2591\n",
      "Epoch 903/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2579\n",
      "Epoch 904/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2585\n",
      "Epoch 905/1000\n",
      "975/975 [==============================] - 10s 10ms/step - loss: 0.2596\n",
      "Epoch 906/1000\n",
      "975/975 [==============================] - 11s 10ms/step - loss: 0.2580\n",
      "Epoch 907/1000\n",
      "975/975 [==============================] - 11s 10ms/step - loss: 0.2605\n",
      "Epoch 908/1000\n",
      "975/975 [==============================] - 11s 9ms/step - loss: 0.2580\n",
      "Epoch 909/1000\n",
      "975/975 [==============================] - 11s 10ms/step - loss: 0.2590\n",
      "Epoch 910/1000\n",
      "975/975 [==============================] - 11s 10ms/step - loss: 0.2588\n",
      "Epoch 911/1000\n",
      "975/975 [==============================] - 11s 10ms/step - loss: 0.2586\n",
      "Epoch 912/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2579\n",
      "Epoch 913/1000\n",
      "975/975 [==============================] - 11s 10ms/step - loss: 0.2592\n",
      "Epoch 914/1000\n",
      "975/975 [==============================] - 11s 10ms/step - loss: 0.2589\n",
      "Epoch 915/1000\n",
      "975/975 [==============================] - 10s 10ms/step - loss: 0.2586\n",
      "Epoch 916/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2580\n",
      "Epoch 917/1000\n",
      "975/975 [==============================] - 11s 10ms/step - loss: 0.2598\n",
      "Epoch 918/1000\n",
      "975/975 [==============================] - 11s 10ms/step - loss: 0.2585\n",
      "Epoch 919/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2587\n",
      "Epoch 920/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2587\n",
      "Epoch 921/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2582\n",
      "Epoch 922/1000\n",
      "975/975 [==============================] - 11s 10ms/step - loss: 0.2587\n",
      "Epoch 923/1000\n",
      "975/975 [==============================] - 12s 11ms/step - loss: 0.2587\n",
      "Epoch 924/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2596\n",
      "Epoch 925/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2579\n",
      "Epoch 926/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2584\n",
      "Epoch 927/1000\n",
      "975/975 [==============================] - 11s 10ms/step - loss: 0.2587\n",
      "Epoch 928/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2580\n",
      "Epoch 929/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2579\n",
      "Epoch 930/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2584\n",
      "Epoch 931/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2584\n",
      "Epoch 932/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2584\n",
      "Epoch 933/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2584\n",
      "Epoch 934/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2576\n",
      "Epoch 935/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2589\n",
      "Epoch 936/1000\n",
      "975/975 [==============================] - 11s 10ms/step - loss: 0.2581\n",
      "Epoch 937/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2582\n",
      "Epoch 938/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2589\n",
      "Epoch 939/1000\n",
      "975/975 [==============================] - 11s 9ms/step - loss: 0.2580\n",
      "Epoch 940/1000\n",
      "975/975 [==============================] - 11s 10ms/step - loss: 0.2580\n",
      "Epoch 941/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2582\n",
      "Epoch 942/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2585\n",
      "Epoch 943/1000\n",
      "975/975 [==============================] - 11s 10ms/step - loss: 0.2583\n",
      "Epoch 944/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2580\n",
      "Epoch 945/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2585\n",
      "Epoch 946/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2578\n",
      "Epoch 947/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2584\n",
      "Epoch 948/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2588\n",
      "Epoch 949/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2581\n",
      "Epoch 950/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2571\n",
      "Epoch 951/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2598\n",
      "Epoch 952/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2584\n",
      "Epoch 953/1000\n",
      "975/975 [==============================] - 10s 10ms/step - loss: 0.2573\n",
      "Epoch 954/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2583\n",
      "Epoch 955/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2589\n",
      "Epoch 956/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2573\n",
      "Epoch 957/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2585\n",
      "Epoch 958/1000\n",
      "975/975 [==============================] - 10s 10ms/step - loss: 0.2571\n",
      "Epoch 959/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2576\n",
      "Epoch 960/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2594\n",
      "Epoch 961/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2573\n",
      "Epoch 962/1000\n",
      "975/975 [==============================] - 10s 10ms/step - loss: 0.2585\n",
      "Epoch 963/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2578\n",
      "Epoch 964/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2576\n",
      "Epoch 965/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2587\n",
      "Epoch 966/1000\n",
      "975/975 [==============================] - 10s 10ms/step - loss: 0.2576\n",
      "Epoch 967/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2582\n",
      "Epoch 968/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2576\n",
      "Epoch 969/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2585\n",
      "Epoch 970/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2575\n",
      "Epoch 971/1000\n",
      "975/975 [==============================] - 11s 10ms/step - loss: 0.2575\n",
      "Epoch 972/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2582\n",
      "Epoch 973/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2578\n",
      "Epoch 974/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2575\n",
      "Epoch 975/1000\n",
      "975/975 [==============================] - 11s 10ms/step - loss: 0.2583\n",
      "Epoch 976/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2584\n",
      "Epoch 977/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2574\n",
      "Epoch 978/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2583\n",
      "Epoch 979/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2577\n",
      "Epoch 980/1000\n",
      "975/975 [==============================] - 10s 10ms/step - loss: 0.2579\n",
      "Epoch 981/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2577\n",
      "Epoch 982/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2575\n",
      "Epoch 983/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2584\n",
      "Epoch 984/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2580\n",
      "Epoch 985/1000\n",
      "975/975 [==============================] - 10s 10ms/step - loss: 0.2579\n",
      "Epoch 986/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2574\n",
      "Epoch 987/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2579\n",
      "Epoch 988/1000\n",
      "975/975 [==============================] - 11s 9ms/step - loss: 0.2579\n",
      "Epoch 989/1000\n",
      "975/975 [==============================] - 11s 10ms/step - loss: 0.2571\n",
      "Epoch 990/1000\n",
      "975/975 [==============================] - 10s 10ms/step - loss: 0.2588\n",
      "Epoch 991/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2567\n",
      "Epoch 992/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2577\n",
      "Epoch 993/1000\n",
      "975/975 [==============================] - 10s 10ms/step - loss: 0.2583\n",
      "Epoch 994/1000\n",
      "975/975 [==============================] - 10s 10ms/step - loss: 0.2578\n",
      "Epoch 995/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2574\n",
      "Epoch 996/1000\n",
      "975/975 [==============================] - 10s 8ms/step - loss: 0.2575\n",
      "Epoch 997/1000\n",
      "975/975 [==============================] - 10s 9ms/step - loss: 0.2576\n",
      "Epoch 998/1000\n",
      "975/975 [==============================] - 10s 10ms/step - loss: 0.2576\n",
      "Epoch 999/1000\n",
      "975/975 [==============================] - 9s 9ms/step - loss: 0.2587\n",
      "Epoch 1000/1000\n",
      "975/975 [==============================] - 9s 8ms/step - loss: 0.2575\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51d03783",
   "metadata": {
    "id": "51d03783"
   },
   "outputs": [],
   "source": [
    "class Char_generator(tf.keras.Model):\n",
    "  def __init__(self, model, get_char_from_id, get_id_from_char):\n",
    "    super().__init__()\n",
    "    self.model = model\n",
    "    self.get_char_from_id = get_char_from_id\n",
    "    self.get_id_from_char = get_id_from_char\n",
    "\n",
    "    # logic to not print [unk]\n",
    "    skip_ids = self.get_id_from_char(['[UNK]'])[:, None]\n",
    "    mask = tf.SparseTensor(values=[-float('inf')]*len(skip_ids),indices=skip_ids,dense_shape=[len(get_id_from_char.get_vocabulary())])\n",
    "    self.mask = tf.sparse.to_dense(mask)\n",
    "\n",
    "  @tf.function\n",
    "  def predict_next_char(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    string_input = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.get_id_from_char(string_input).to_tensor()\n",
    "\n",
    "    # predict the output for given input string\n",
    "    output, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last char.\n",
    "    output = output[:, -1, :]\n",
    "    \n",
    "    output = output + self.mask\n",
    "\n",
    "    output_ids = tf.random.categorical(output, num_samples=1)\n",
    "    output_ids = tf.squeeze(output_ids, axis=-1)\n",
    "\n",
    "    # convert to string from ids\n",
    "    output_string = self.get_char_from_id(output_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return output_string, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "MJyZlMeF5MIk",
   "metadata": {
    "id": "MJyZlMeF5MIk"
   },
   "outputs": [],
   "source": [
    "char_gen_model = Char_generator(model, get_char_from_id, get_id_from_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "GszQ7F4b5jN6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GszQ7F4b5jN6",
    "outputId": "fe9b8576-a096-4d37-c59d-7b18a8a1a316"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inspired by youth,\n",
      "and sweets grown common lose their dear victories once foiled,\n",
      "is from the book of honour razed quite,\n",
      "and all the rest forgot for which he toiled:\n",
      "then happy i that love and am beloved\n",
      "where i may not remove nor be removed.\n",
      "  \n",
      "lord of my love, to whom in vew shore away,\n",
      "death's second self that seals up all in rest.  \n",
      "o surn dear head, else true, love twinness shaken\n",
      "as i by yours, y'have passed a hell of time,\n",
      "and i a tyrant have no leisure taken\n",
      "to weigh how once i suffered in your crime.\n",
      "o that our night of woe might have remembered\n",
      "my deepest sense, how hard true sorrow hits,\n",
      "and soon to you, as you to medic women's spring: forth where,\n",
      "and consument that your love taught it this alchemy?\n",
      "to make of monsters, and things indigest,\n",
      "such cherubins as your sweet self resembote,\n",
      "but that sorrt thou lead away,\n",
      "if thou wouldst almoward of a conquered woe,\n",
      "give not a windy night a woman coloured ill.\n",
      "to win thy presence is gracious and kind,\n",
      "or to thy self at least kind-hearted prove, \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 8.803258419036865\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['inspired by you'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(400):\n",
    "  next_char, states = char_gen_model.predict_next_char(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "  next_char = tf.constant(tf.strings.join(result))\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "_PVasWGeCx2l",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_PVasWGeCx2l",
    "outputId": "fb2c5e4c-29ba-4163-9752-64e7c4f96b33"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "\n",
    "\n",
    "# Specify export directory and use tensorflow to save your_model\n",
    "export_dir = './saved_model'\n",
    "tf.saved_model.save(model, export_dir=export_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "wvGtVirBC8Cl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wvGtVirBC8Cl",
    "outputId": "2e97aaa8-f9e3-4f5a-8f1e-b3a17ec377ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on History in module keras.callbacks object:\n",
      "\n",
      "class History(Callback)\n",
      " |  Callback that records events into a `History` object.\n",
      " |  \n",
      " |  This callback is automatically applied to\n",
      " |  every Keras model. The `History` object\n",
      " |  gets returned by the `fit` method of models.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  >>> model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\n",
      " |  >>> model.compile(tf.keras.optimizers.SGD(), loss='mse')\n",
      " |  >>> history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\n",
      " |  ...                     epochs=10, verbose=1)\n",
      " |  >>> print(history.params)\n",
      " |  {'verbose': 1, 'epochs': 10, 'steps': 1}\n",
      " |  >>> # check the keys of history object\n",
      " |  >>> print(history.history.keys())\n",
      " |  dict_keys(['loss'])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      History\n",
      " |      Callback\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  on_epoch_end(self, epoch, logs=None)\n",
      " |      Called at the end of an epoch.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run. This function should\n",
      " |      only be called during TRAIN mode.\n",
      " |      \n",
      " |      Args:\n",
      " |          epoch: Integer, index of epoch.\n",
      " |          logs: Dict, metric results for this training epoch, and for the\n",
      " |            validation epoch if validation is performed. Validation result\n",
      " |            keys are prefixed with `val_`. For training epoch, the values of\n",
      " |            the `Model`'s metrics are returned. Example:\n",
      " |            `{'loss': 0.2, 'accuracy': 0.7}`.\n",
      " |  \n",
      " |  on_train_begin(self, logs=None)\n",
      " |      Called at the beginning of training.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this\n",
      " |            method but that may change in the future.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Callback:\n",
      " |  \n",
      " |  on_batch_begin(self, batch, logs=None)\n",
      " |      A backwards compatibility alias for `on_train_batch_begin`.\n",
      " |  \n",
      " |  on_batch_end(self, batch, logs=None)\n",
      " |      A backwards compatibility alias for `on_train_batch_end`.\n",
      " |  \n",
      " |  on_epoch_begin(self, epoch, logs=None)\n",
      " |      Called at the start of an epoch.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run. This function should\n",
      " |      only be called during TRAIN mode.\n",
      " |      \n",
      " |      Args:\n",
      " |          epoch: Integer, index of epoch.\n",
      " |          logs: Dict. Currently no data is passed to this argument for this\n",
      " |            method but that may change in the future.\n",
      " |  \n",
      " |  on_predict_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a batch in `predict` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every\n",
      " |      `N` batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Currently no data is passed to this argument for this\n",
      " |            method but that may change in the future.\n",
      " |  \n",
      " |  on_predict_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a batch in `predict` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every\n",
      " |      `N` batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  on_predict_begin(self, logs=None)\n",
      " |      Called at the beginning of prediction.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this\n",
      " |            method but that may change in the future.\n",
      " |  \n",
      " |  on_predict_end(self, logs=None)\n",
      " |      Called at the end of prediction.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this\n",
      " |            method but that may change in the future.\n",
      " |  \n",
      " |  on_test_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a batch in `evaluate` methods.\n",
      " |      \n",
      " |      Also called at the beginning of a validation batch in the `fit`\n",
      " |      methods, if validation data is provided.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every\n",
      " |      `N` batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Currently no data is passed to this argument for this\n",
      " |            method but that may change in the future.\n",
      " |  \n",
      " |  on_test_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a batch in `evaluate` methods.\n",
      " |      \n",
      " |      Also called at the end of a validation batch in the `fit`\n",
      " |      methods, if validation data is provided.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every\n",
      " |      `N` batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  on_test_begin(self, logs=None)\n",
      " |      Called at the beginning of evaluation or validation.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this\n",
      " |            method but that may change in the future.\n",
      " |  \n",
      " |  on_test_end(self, logs=None)\n",
      " |      Called at the end of evaluation or validation.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently the output of the last call to\n",
      " |            `on_test_batch_end()` is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_train_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a training batch in `fit` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every\n",
      " |      `N` batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Currently no data is passed to this argument for this\n",
      " |            method but that may change in the future.\n",
      " |  \n",
      " |  on_train_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a training batch in `fit` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every\n",
      " |      `N` batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  on_train_end(self, logs=None)\n",
      " |      Called at the end of training.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently the output of the last call to\n",
      " |            `on_epoch_end()` is passed to this argument for this method but\n",
      " |            that may change in the future.\n",
      " |  \n",
      " |  set_model(self, model)\n",
      " |  \n",
      " |  set_params(self, params)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Callback:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "_Mhuoa57v4pc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Mhuoa57v4pc",
    "outputId": "6d613ed6-06f9-4221-95ab-19f122837b8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the little love-god lying once asleep,\r\n",
      "laid by his side his heart-inflaming brand,\r\n",
      "whilst many nymphs that vowed chaste life to keep,\r\n",
      "came tripping by, but in her maiden hand,\r\n",
      "the fairest votary took up that fire,\r\n",
      "which many legions of true hearts had warmed,\r\n",
      "and so the general of hot desire,\r\n",
      "was sleeping by a virgin hand disarmed.\r\n",
      "this brand she quenched in a cool well by,\r\n",
      "which from love's fire took heat perpetual,\r\n",
      "growing a bath and healthful remedy,\r\n",
      "for men discased, but i my mistress' thrall,\r\n",
      "came there for cure and this by that i prove,  \r\n",
      "love's fire heats water, water cools not love.\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hold_outtext = open(\"hold-out.txt\", 'rb').read().decode(encoding='utf-8')\n",
    "hold_outtext=hold_outtext.lower()\n",
    "print(hold_outtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "SqIu3yzfx8LK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SqIu3yzfx8LK",
    "outputId": "296419d4-f228-41f7-c1f0-d3e164359cfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[32 20 17  2 24 21 32 32 24 17  2 24 27 34 17  8 19 27 16  2 24 37 21 26\n",
      " 19  2 27 26 15 17  2 13 31 24 17 17 28  7  0  1 24 13 21 16  2 14 37  2\n",
      " 20 21 31  2 31 21 16 17  2 20 21 31  2 20 17 13 30 32  8 21 26 18 24 13\n",
      " 25 21 26 19  2 14 30 13 26 16  7  0  1 35 20 21 24 31 32  2 25 13 26 37\n",
      "  2 26 37 25 28 20 31  2 32 20 13 32  2 34 27 35 17 16  2 15 20 13 31 32\n",
      " 17  2 24 21 18 17  2 32 27  2 23 17 17 28  7  0  1 15 13 25 17  2 32 30\n",
      " 21 28 28 21 26 19  2 14 37  7  2 14 33 32  2 21 26  2 20 17 30  2 25 13\n",
      " 21 16 17 26  2 20 13 26 16  7  0  1 32 20 17  2 18 13 21 30 17 31 32  2\n",
      " 34 27 32 13 30 37  2 32 27 27 23  2 33 28  2 32 20 13 32  2 18 21 30 17\n",
      "  7  0  1 35 20 21 15 20  2 25 13 26 37  2 24 17 19 21 27 26 31  2 27 18\n",
      "  2 32 30 33 17  2 20 17 13 30 32 31  2 20 13 16  2 35 13 30 25 17 16  7\n",
      "  0  1 13 26 16  2 31 27  2 32 20 17  2 19 17 26 17 30 13 24  2 27 18  2\n",
      " 20 27 32  2 16 17 31 21 30 17  7  0  1 35 13 31  2 31 24 17 17 28 21 26\n",
      " 19  2 14 37  2 13  2 34 21 30 19 21 26  2 20 13 26 16  2 16 21 31 13 30\n",
      " 25 17 16  9  0  1 32 20 21 31  2 14 30 13 26 16  2 31 20 17  2 29 33 17\n",
      " 26 15 20 17 16  2 21 26  2 13  2 15 27 27 24  2 35 17 24 24  2 14 37  7\n",
      "  0  1 35 20 21 15 20  2 18 30 27 25  2 24 27 34 17  4 31  2 18 21 30 17\n",
      "  2 32 27 27 23  2 20 17 13 32  2 28 17 30 28 17 32 33 13 24  7  0  1 19\n",
      " 30 27 35 21 26 19  2 13  2 14 13 32 20  2 13 26 16  2 20 17 13 24 32 20\n",
      " 18 33 24  2 30 17 25 17 16 37  7  0  1 18 27 30  2 25 17 26  2 16 21 31\n",
      " 15 13 31 17 16  7  2 14 33 32  2 21  2 25 37  2 25 21 31 32 30 17 31 31\n",
      "  4  2 32 20 30 13 24 24  7  0  1 15 13 25 17  2 32 20 17 30 17  2 18 27\n",
      " 30  2 15 33 30 17  2 13 26 16  2 32 20 21 31  2 14 37  2 32 20 13 32  2\n",
      " 21  2 28 30 27 34 17  7  2  2  0  1 24 27 34 17  4 31  2 18 21 30 17  2\n",
      " 20 17 13 32 31  2 35 13 32 17 30  7  2 35 13 32 17 30  2 15 27 27 24 31\n",
      "  2 26 27 32  2 24 27 34 17  9  0  1], shape=(612,), dtype=int64)\n",
      "612\n",
      "Number of training Samples =  191\n",
      "Vocab Size 38 \n",
      "tf.Tensor(-2, shape=(), dtype=int64)\n",
      "Input : b'the little love-god lying once asleep,[UNK]\\n'\n",
      "\n",
      "One Hot encoding representation for above text tf.Tensor(\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]], shape=(40, 38), dtype=float32)\n",
      "\n",
      "\n",
      "Target: b'he little love-god lying once asleep,[UNK]\\nl'\n",
      "\n",
      "One Hot encoding representation for above text tf.Tensor(\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]], shape=(40, 38), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "text_ids = get_id_from_char(tf.strings.unicode_split(hold_outtext, 'UTF-8'))\n",
    "print(text_ids)\n",
    "print(len(hold_outtext))\n",
    "dataset = tf.data.Dataset.from_tensor_slices(text_ids)\n",
    "\n",
    "## Create dataset of datasets with a specific window and shift size\n",
    "dataset = dataset.window(size=window+1,shift=stride, drop_remainder=True)\n",
    "\n",
    "## Function to flat the dataset from the datasets\n",
    "def flat_function(batch):\n",
    "    return batch.batch(window+1, drop_remainder=True)\n",
    "\n",
    "## Use above function to apply on the loaded dataset\n",
    "dataset_sequence = dataset.flat_map(flat_function)\n",
    "\n",
    "count = 0\n",
    "for x in dataset_sequence:\n",
    "  count+=1\n",
    "print(\"Number of training Samples = \",count)\n",
    "print(f'Vocab Size {len(vocab)} ')\n",
    "\n",
    "dataset = dataset_sequence.map(split_input_target)\n",
    "num_records = tf.data.experimental.cardinality(dataset)\n",
    "\n",
    "# Print the number of records\n",
    "print(num_records)\n",
    "\n",
    "for sample_input, sample_target in dataset.take(1):\n",
    "    ids = tf.argmax(sample_input, axis=1)\n",
    "    print(\"Input :\", generate_text(ids).numpy())\n",
    "    print(\"\\nOne Hot encoding representation for above text\",sample_input)\n",
    "    ids = tf.argmax(sample_target, axis=1)\n",
    "    print(\"\\n\\nTarget:\", generate_text(ids).numpy())\n",
    "    print(\"\\nOne Hot encoding representation for above text\",sample_target)\n",
    "\n",
    "validation_dataset = sequences.map(lambda_split)\n",
    "validation_dataset = (validation_dataset.shuffle(buffer).batch(batch, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ZsBgVD4Ezjr9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZsBgVD4Ezjr9",
    "outputId": "a948d1d7-2181-4f4d-eb25-5f9444a04fe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 - 0s - loss: 7.4239 - 92ms/epoch - 18ms/step\n",
      "Test Perplexity: 1675.514115646483\n"
     ]
    }
   ],
   "source": [
    "# model evaluation and print perplexity\n",
    "test_loss = model.evaluate(validation_dataset, verbose=2)\n",
    "print('Test Perplexity:', np.exp(test_loss))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
